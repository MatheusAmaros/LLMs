{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FCIQH-bgkeC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate peft trl bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "pw9J6tHMogoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyZ7t-nRgmTW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"mbpp\")\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transfer the model to the GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "cCRx9DYgs2gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The padding token is set to the unknown token.\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "# The ID of the padding token is set to the ID of the unknown token.\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "\n",
        "# The padding side is set to 'left', meaning that padding tokens will be added to the left (start) of the sequence.\n",
        "tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "vHlvefQ8wMBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9GkgvXBUo0O"
      },
      "outputs": [],
      "source": [
        "max_input_length = 256\n",
        "max_target_length = 512\n",
        "\n",
        "def preprocess_examples(examples):\n",
        "  codes = examples['code']\n",
        "  texts = examples['text']\n",
        "\n",
        "  model_inputs = tokenizer(texts, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "  labels = tokenizer(codes, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "  label_list = []\n",
        "  for label in labels:\n",
        "    label_list.append([item if item != tokenizer.pad_token_id else -100 for item in label])\n",
        "\n",
        "  model_inputs['labels'] = label_list\n",
        "\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0evgMdUaWsFD"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(preprocess_examples, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amPfOiFBXC98"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
        "\n",
        "args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=1000,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=512,\n",
        "        tokenizer=tokenizer,\n",
        "        args=args\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = trainer.state.log_history\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss and step values\n",
        "steps = [entry['step'] for entry in data if 'loss' in entry]\n",
        "losses = [entry['loss'] for entry in data if 'loss' in entry]\n",
        "\n",
        "# Plot the loss vs. steps\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot( steps,losses,  linestyle='-', color='b')\n",
        "plt.title('train/loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zl21dWLAdeq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIpX1UEYTa0H"
      },
      "outputs": [],
      "source": [
        "model_dir = \"/content/finetuned_gpt_mbpp\"  # Path to the checkpoint directory\n",
        "tokenizer_dir = \"/content/finetuned_gpt_mbpp_tk/\"  # Path to directory with the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s4PPykuTYjd"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCznQeEygt7Z"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "WkncjbueyAtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the input text and transfer it to the GPU\n",
        "input_text = f\"Solve this problem in python: {dataset['validation']['text'][10]}\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "# Generate the output\n",
        "output = model.generate(input_ids, max_length=512, repetition_penalty=2.0)\n",
        "\n",
        "# Decode the generated output (no need to move it to CPU for decoding)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the input and output\n",
        "print(input_text)\n",
        "print(\"Generated code:\")\n",
        "print(generated_text)\n",
        "print(\"Expected code:\")\n",
        "print(dataset['validation']['code'][10])\n"
      ],
      "metadata": {
        "id": "KOq9z8LxrLyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}